{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_scripts import load_ipython_item, load_ipython_log\n",
    "\n",
    "data_path = Path(\"data/\")\n",
    "\n",
    "item = load_ipython_item(data_path)\n",
    "log, features = load_ipython_log(data_path, data_path / \"edulint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def find_example_message_from_message_code(messages, message_code):\n",
    "    pattern = r'\"[^\"]*{}[^\"]*\"'.format(re.escape(message_code))\n",
    "    message = re.search(pattern, messages)\n",
    "    if message is None:\n",
    "        return f'\"{message_code}_unknown\"'\n",
    "    return message.group()\n",
    "\n",
    "\n",
    "with open(data_path / \"edulint\" / \"results.txt\") as f:\n",
    "    messages = f.read()\n",
    "    feature_descriptions = {}\n",
    "    for feature_name in features:\n",
    "        feature_descriptions[feature_name] = find_example_message_from_message_code(\n",
    "            messages, feature_name.upper()\n",
    "        )\n",
    "\n",
    "feature_descriptions['r1705'] = 'Unnecessary elif after return'\n",
    "feature_descriptions['c0103'] = 'Naming style violation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.DataFrame(\n",
    "    np.vstack(log[\"linter_messages\"]), columns=features, index=log.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages.div(log['answer'].apply(lambda x: len(x.split())), axis=0)\n",
    "messages = messages == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_submission_for_messages(*args, idx=0):\n",
    "    for arg in args:\n",
    "        print(f'Total: {messages[arg].sum()}, Description: {feature_descriptions[arg]}')\n",
    "    print('-------------------------------------------------------------')\n",
    "    mask = messages[args[0]].copy()\n",
    "    if len(args) > 1:\n",
    "        for arg in args[1:]:\n",
    "            if messages[arg].dtype == bool:\n",
    "                mask &= messages[arg]\n",
    "            else:\n",
    "                mask += messages[arg]\n",
    "        print(f'Intersection total: {mask.sum()}')\n",
    "    print(log[mask > 0].iloc[idx][\"answer\"])\n",
    "\n",
    "example_submission_for_messages(\"w293\", 'e303', idx=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = messages.sum(axis=0).sort_values()\n",
    "fig = px.bar(x=[feature_descriptions[i] for i in counts.index], y=counts / counts.sum())\n",
    "fig.update_layout(\n",
    "    title=f\"Selected linter messages and the frequency of their presence (in total {counts.sum()} submissions)\",\n",
    "    xaxis_title=\"Message\",\n",
    "    yaxis_title=\"Frequency of submissions\",\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correlations = messages.corr()\n",
    "\n",
    "fig = px.imshow(\n",
    "    feature_correlations,\n",
    "    labels=dict(x=\"Message codes\", y=\"Messages\", color=\"Correlation\"),\n",
    "    x=feature_correlations.columns,\n",
    "    y=feature_correlations.columns,\n",
    "    color_continuous_scale=\"Viridis\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title=\"Feature Correlogram Before Preprocessing\",\n",
    "    yaxis=dict(\n",
    "        tickvals=list(range(len(feature_correlations.columns))),\n",
    "        ticktext=[feature_descriptions[col] for col in feature_correlations.columns],\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "feature_distances = 1 - np.abs(feature_correlations)\n",
    "dist_linkage = hierarchy.ward(squareform(feature_distances))\n",
    "dendro = hierarchy.dendrogram(\n",
    "    dist_linkage, labels=messages.columns.to_list(), leaf_rotation=90\n",
    ")\n",
    "plt.title('Dendrogram of Feature Correlations Before Preprocessing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_messages(description, *args):\n",
    "    new_name = '+'.join(args)\n",
    "    messages[new_name] = messages[args[0]].copy()\n",
    "    for arg in args[1:]:\n",
    "        if messages[arg].dtype == bool:\n",
    "            messages[new_name] |= messages[arg]\n",
    "        else:\n",
    "            messages[new_name] += messages[arg]\n",
    "    feature_descriptions[new_name] = description\n",
    "    messages.drop(list(args), axis=1, inplace=True)\n",
    "\n",
    "combine_messages('Bad inline comment.', 'e261', 'e262')\n",
    "combine_messages('Redefining var/foo.', 'f811', 'e0102')\n",
    "combine_messages('Spaces in indentation.', 'e101', 'w191')\n",
    "combine_messages('Bad indentation.', 'e111', 'e117')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_messages('No spacing between blocks.', 'e302', 'e305')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E305 and E302 both show students debugging / trying to calculate the answer manually / very confused about the basic principles of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correlations = messages.corr()\n",
    "feature_distances = 1 - np.abs(messages.corr())\n",
    "\n",
    "fig = px.imshow(\n",
    "    feature_correlations,\n",
    "    labels=dict(x=\"Messages\", y=\"Message codes\", color=\"Correlation\"),\n",
    "    x=feature_correlations.columns,\n",
    "    y=feature_correlations.columns,\n",
    "    color_continuous_scale=\"Viridis\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title=\"Feature Correlogram After Preprocessing\",\n",
    "    yaxis=dict(\n",
    "        tickvals=list(range(len(feature_correlations.columns))),\n",
    "        ticktext=[feature_descriptions[col] for col in feature_correlations.columns],\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_distances = 1 - np.abs(feature_correlations)\n",
    "dist_linkage = hierarchy.ward(squareform(feature_distances))\n",
    "dendro = hierarchy.dendrogram(\n",
    "    dist_linkage, labels=messages.columns.to_list(), leaf_rotation=90\n",
    ")\n",
    "plt.title('Dendrogram of Feature Correlations After Preprocessing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How likely are users to repeat each of the detected mistakes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How severe are the messages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naively predicting whether the submission was unsuccessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "scores = {}\n",
    "for msg in messages.columns:\n",
    "    scores[msg] = matthews_corrcoef(messages[msg], 1 - log[\"correct\"])\n",
    "\n",
    "labels, score = zip(*sorted(scores.items(), key=lambda x: x[1]))\n",
    "fig = px.bar(x=score, y=[feature_descriptions[label] for label in labels])\n",
    "fig.update_layout(\n",
    "    title=\"Correlation between the presence of each message and whether the submission was unsuccessful\",\n",
    "    xaxis_title=\"Messages\",\n",
    "    yaxis_title=\"MCC\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_scores = pd.Series(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import r_regression\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "r_scores = sorted(list(zip(messages.columns, r_regression(messages, log[\"correct\"]))),key=lambda x: x[1])\n",
    "plt.barh(*zip(*r_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "def pretty_confusion_matrix(cm):\n",
    "    labels = [\n",
    "        f\"{label}\\n{count}\"\n",
    "        for label, count in zip(\n",
    "            [\"True Negatives\", \"False Positives\", \"False Negatives\", \"True Positives\"],\n",
    "            [cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]],\n",
    "        )\n",
    "    ]\n",
    "    # Create confusion matrix table\n",
    "    cm_table = ff.create_annotated_heatmap(\n",
    "        z=cm,\n",
    "        x=[\"Predicted 0\", \"Predicted 1\"],\n",
    "        y=[\"Actual 0\", \"Actual 1\"],\n",
    "        colorscale=\"Blues\",\n",
    "    )\n",
    "    cm_table.update_layout(\n",
    "        title_text=\"Confusion Matrix\",\n",
    "        xaxis=dict(title=\"Predicted label\"),\n",
    "        yaxis=dict(title=\"True label\"),\n",
    "    )\n",
    "\n",
    "    # Add labels to the confusion matrix\n",
    "    for i in range(len(cm_table.layout.annotations)):\n",
    "        cm_table.layout.annotations[i].text = labels[i]\n",
    "\n",
    "    # Show confusion matrix\n",
    "    cm_table.show()\n",
    "\n",
    "\n",
    "def confusion_matrix_for_message(code):\n",
    "    cm = confusion_matrix(log[\"correct\"], messages[code])\n",
    "    pretty_confusion_matrix(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    messages, 1 - log[\"correct\"], test_size=0.33, random_state=42\n",
    ")\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "scores = np.round(cross_val_score(clf, X_train, y_train, cv=5, scoring=\"matthews_corrcoef\"), 3)\n",
    "print(f\"Scores on validation data for each fold: {scores}.\")\n",
    "print(f\"Score on the holdout test set: {matthews_corrcoef(y_test, clf.predict(X_test))}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print('Precision: ', precision_score(y_test, pred))\n",
    "print('Recall: ', recall_score(y_test, pred))\n",
    "pretty_confusion_matrix(confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "importance = permutation_importance(clf, X_test, y_test, scoring=\"matthews_corrcoef\", random_state=42)\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(len(messages.columns)):\n",
    "    if importance.importances_mean[i] > 0.003:\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=importance.importances[i],\n",
    "                name=feature_descriptions[messages.columns[i]],\n",
    "                hoverinfo='name',\n",
    "                hoverlabel = dict(namelength = -1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Permutation Feature Importance for a Model Trained to Predict the Success of a Submission from the presence of Linter Messages\",\n",
    "    yaxis_title=\"Features\",\n",
    "    xaxis_title=\"Importance\",\n",
    "    showlegend=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is there positive correlation for some of the messages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of session is defined as sucessful submission, changing to a different task or not submitting for more than 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log = []\n",
    "for user in np.unique(log['user']):\n",
    "    # get the user history and make sure the values are sorted\n",
    "    user_history = log[log['user'] == user].sort_values('time')\n",
    "\n",
    "    # find the session breakpoints\n",
    "    user_history['sessionEnd'] = user_history['correct']        | \\\n",
    "        user_history['item'].ne(user_history['item'].shift(-1)) | \\\n",
    "        (user_history['time'].diff() > pd.Timedelta(minutes=20))\n",
    "\n",
    "    # propagate the session success backwards\n",
    "    user_history['sessionSucess'] = False # to correctly initialize the type\n",
    "    for index, row in user_history.iloc[::-1].iterrows():\n",
    "        if row['sessionEnd']:\n",
    "            sucess = row['correct']\n",
    "        user_history.at[index, 'sessionSucess'] = sucess\n",
    "    new_log.append(user_history)\n",
    "\n",
    "log = pd.concat(new_log).sort_index()\n",
    "messages.sort_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the success of the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if messages.dtypes.apply(lambda x: x == bool).all():\n",
    "    msg_scores = {}\n",
    "    for msg in messages.columns:\n",
    "        msg_scores[feature_descriptions[msg]] = matthews_corrcoef(messages[msg], 1 - log[\"sessionSucess\"])\n",
    "\n",
    "    x, y = zip(*sorted(msg_scores.items(), key=lambda x: x[1]))\n",
    "    fig = px.bar(x=x, y=y)\n",
    "    fig.update_layout(\n",
    "        title=\"Correlation between the presence of each message and whether the session was unsuccessful\",\n",
    "        xaxis_title=\"Messages\",\n",
    "        yaxis_title=\"MCC\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import r_regression\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "r_scores = sorted(list(zip(messages.columns, r_regression(messages, log[\"sessionSucess\"]))),key=lambda x: x[1])\n",
    "plt.barh(*zip(*r_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages = messages[messages.columns[np.asarray(list(msg_scores.values())) > 0.01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    messages, 1 - log[\"sessionSucess\"], test_size=0.33, random_state=0\n",
    ")\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "scores = np.round(cross_val_score(clf, X_train, y_train, cv=5, scoring=\"matthews_corrcoef\"), 3)\n",
    "print(f\"Scores on validation data for each fold: {scores}.\")\n",
    "print(f\"Score on the holdout test set: {matthews_corrcoef(y_test, clf.predict(X_test))}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)\n",
    "print('Precision: ', precision_score(y_test, pred))\n",
    "print('Recall: ', recall_score(y_test, pred))\n",
    "pretty_confusion_matrix(confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at only last N submissions is a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time until completion / the next submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at sudents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the messages distinguish successful students from unsuccessful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "df = []\n",
    "index = []\n",
    "for group in log['item'].unique():\n",
    "    incorrect = log[log['item'] == group]['correct']\n",
    "    if len(incorrect) < 500:\n",
    "        continue\n",
    "    df.append(messages[log['item'] == group].apply(lambda x: matthews_corrcoef(x, incorrect), axis=0))\n",
    "    index.append(group)\n",
    "\n",
    "df = pd.DataFrame(np.vstack(df), index=[item.loc[i]['name'] for i in index], columns=messages.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def entropy_for_group(group):\n",
    "    return entropy(group.value_counts())\n",
    "\n",
    "df = []\n",
    "index = []\n",
    "for value, count in log['item'].value_counts().items():\n",
    "    if count < 800:\n",
    "        continue\n",
    "    df.append(messages[log['item'] == value].apply(entropy_for_group))\n",
    "    index.append(value)\n",
    "\n",
    "df = pd.DataFrame(np.vstack(df), index=[item.loc[i]['name'] for i in index], columns=[feature_descriptions[msg][:35] for msg in messages.columns]).T\n",
    "df['Mean Entropy'] = df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "best_pos_labels = [feature_descriptions[msg][:35] for msg in correlation_scores.sort_values(ascending=False)[:5].index]\n",
    "best_neg_labels = [feature_descriptions[msg][:35] for msg in correlation_scores.sort_values()[:5].index]\n",
    "\n",
    "sns.set_theme(rc={'figure.figsize':(15,8)})\n",
    "g = sns.heatmap(df.loc[df.mean(axis=1).sort_values(ascending=False)[:25].index], vmin=0, vmax=1).set_title('Entropy of Linter Messages for the Most Frequent Items')\n",
    "for tick_label in g.axes.get_yticklabels():\n",
    "    if tick_label.get_text() in best_pos_labels:\n",
    "        tick_label.set_color(\"red\")\n",
    "    if tick_label.get_text() in best_neg_labels:\n",
    "        tick_label.set_color(\"blue\")\n",
    "\n",
    "for tick_label in g.axes.get_xticklabels():\n",
    "    if tick_label.get_text() == 'Mean Entropy':\n",
    "        tick_label.set_color(\"blue\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
